# ğŸ” Web Search Engine â€“ Indexing & Retrieval System

This repository implements the **indexing and search backend** of a custom-built web search engine, written in Python. It efficiently constructs a disk-based inverted index using partial indexing, and supports ranked document retrieval using TF-IDF and PageRank.

> ğŸ•·ï¸ **Crawling is handled by a separate, dedicated crawler** built from scratch to handle:
> - Politeness rules and robot exclusion
> - Trap detection and loop prevention
> - Subdomain tracking and content filtering  
>
> ğŸ‘‰ Visit the crawler repo here: [Web Crawler Repository](https://github.com/rbelavadi/spacetime-crawler4py) 

## ğŸ“¦ Overview

This repository handles:

- Parsing crawled data from `developer.zip`
- Tokenization, stemming, and stopword filtering
- Partial inverted index creation and memory-efficient merging
- TF-IDF and PageRank scoring
- Boolean and ranked search queries
- Support for boosting based on HTML structure (e.g., titles, headers, bold text)
- Optional support for n-grams, word positions, and anchor text indexing

---

## ğŸ—‚ï¸ Directory Structure

web-search-engine/
â”œâ”€â”€ src/ # All source code and modules
â”œâ”€â”€ index/ # (Ignored) Generated indexes after running the pipeline
â”œâ”€â”€ developer.zip # Crawled data archive (not included in repo)
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

## âš™ï¸ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/rbelavadi/web-search-engine.git
cd web-search-engine
```

### 2. Download the Dataset

The dataset used is developer.zip â€” a zip archive of ~38,000 HTML pages collected from ics.uci.edu and related UCI subdomains.

These pages were crawled using a custom-built crawler (see Web Crawler Repository) and include faculty profiles, research pages, and course descriptions.

ğŸ”— Download here: [Google Drive](https://drive.google.com/file/d/14zgBWE3lDufPLU1pPrYYG7jOFoISiCMG/view?usp=drive_link) link to developer.zip
ğŸ“ Place the file in the root folder of the project.

### 3. Build the Index

This step parses the HTML documents, tokenizes them, and builds a TF-IDFâ€“weighted inverted index.

```bash
python src/buildindex.py
```

### 4. Run the Search Engine

This launches a console-based query interface that supports ranked and Boolean search.

```bash
python src/startMyEngine.py
```

ğŸ“Œ Notes
The index/ folder is ignored via .gitignore and will be regenerated when main.py is run.

developer.zip must be added manually; it contains HTML pages generated by the custom crawler.

The web crawler is a separate project and includes advanced crawling features.
You can find it here: [Web Crawler Repository](https://github.com/rbelavadi/spacetime-crawler4py)

ğŸ‘©â€ğŸ’» Author
Ranjani Belavadi
CS & BIM @ UC Irvine
Python | Systems | Information Retrieval

[GitHub](https://github.com/rbelavadi)
